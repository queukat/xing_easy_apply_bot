personal_information:
  name: "Yaroslav"
  surname: "Rychenkov"
  gender: "male"  # male/female/divers
  date_of_birth: "24/06/1993"
  country: "Georgia"
  city: "Tbilisi"
  address: "4jul, up 72, lamela 1, 8110"
  phone_prefix: "+995"
  phone: "598570802"
  email: "queukat@gmail.com"
  github: "https://github.com/queukat"
  linkedin: "https://www.linkedin.com/in/queukat/"
  xing: "https://www.xing.com/profile/Yar_Ryc/cv"

relocation_preferences:
  open_to_relocation: "Yes"
  target_locations:
    - "London, United Kingdom"
    - "United Kingdom (on-site roles)"

education_details:
  - education_level: "Master's Degree"
    institution: "Udacity"
    field_of_study: "Data Architecture"
    final_evaluation_grade: "N/A"
    start_date: "2020"
    year_of_completion: 2022
    location: "Remote"
    exam:
      Advanced Data Modeling: "4/4"
      Big Data Analytics: "4/4"
      Machine Learning: "4/4"

  - education_level: "Bachelor's Degree"
    institution: "Plekhanov University of Economics"
    field_of_study: "Finance"
    final_evaluation_grade: "N/A"
    start_date: "2016"
    year_of_completion: 2020
    location: "Remote"
    exam:
      Corporate Finance: "A"
      Financial Accounting: "A"
      Investment Analysis: "A-"

experience_details:
  - position: "Senior Data Engineer"
    company: "B2B Remote"
    employment_period: "07/2023 - Present"
    location: "Remote"
    industry: "Big Data"
    key_responsibilities:
      - "Spearheaded the development and optimization of data-driven software solutions, focusing on advanced data engineering practices such as ETL, data integration, and data modeling."
      - "Achieved a 35% improvement in system performance through strategic software optimizations."
      - "Developed novel data processing algorithms, enhancing data accuracy and usability for more informed strategic decision-making."
      - "Implemented data governance frameworks with dbt, automating 85% of data transformations and ensuring compliance with regulatory standards, leading to a 30% increase in data processing accuracy and reducing manual intervention by 50%."
      - "Designed and implemented real-time analytics pipelines leveraging Kafka Streams, providing near-instantaneous insights into critical business metrics."
      - "Enhanced monitoring and logging frameworks, reducing incident response time by 40% and improving overall system reliability."
      - "Built Grafana dashboards on Prometheus metrics to monitor application health and performance."
      - "Collaborated cross-functionally to integrate machine learning models into data workflows, achieving a 20% uplift in predictive analytics accuracy."
      - "Integrated Snowflake as an analytical data warehouse to centralize reporting and accelerate insight delivery."
      - "Prototyped and orchestrated exploratory ETL workloads in Databricks notebooks for data science teams."
    skills_acquired:
      - "Python (3+ years)"
      - "ETL Processes (3+ years)"
      - "Apache Spark (2+ years)"
      - "Airflow (1+ year)"
      - "Data Engineering"
      - "Data Modeling"
      - "Big Data Processing"
      - "SQL"
      - "Cloud Computing (Azure, OCI)"
      - "Snowflake (1+ year)"
      - "Databricks (>=6 months)"
      - "Docker (Containerization)"
      - "Kubernetes (Orchestration)"
      - "CI/CD Pipelines (GitLab CI)"
      - "Data Pipeline Automation"
      - "Data Governance"
      - "Data Security & Privacy"
      - "Machine Learning Integration"
      - "Real-time Data Processing (Kafka)"
      - "API Development"
      - "Microservices Architecture (in the context of data)"
      - "Data Lakes"
      - "Data Quality Monitoring"
      - "Message Queuing (Kafka)"
      - "Version Control (Git, GitLab)"
      - "Scrum/Agile Methodologies"
      - "Data Orchestration Tools (Apache Airflow)"
      - "Performance Tuning & Optimization"
      - "Data Governance & Compliance (GDPR, HIPAA)"
      - "Streaming Analytics"
      - "Automated Testing (Unit, Integration Tests)"
      - "Distributed Computing Systems"
      - "Load Balancing & High Availability"
      - "dbt (Data Build Tool)"
      - "Azure Data Factory"
      - "Data Governance Frameworks"
      - "Apache Zeppelin (2+ years)"
      - "Livy (2+ years)"
      - "ClickHouse (1+ year)"
      - "HDFS (5 years)"
      - "Kafka Connect (2 years)"
      - "JSON/XML Data Handling (3 years)"
      - "Scala (3 years)"
      - "Grafana (Observability)"
      - "Prometheus (Monitoring)"

  - position: "Senior Data Engineer / Project Lead"
    company: "VEON"
    employment_period: "11/2021 - 07/2023"
    location: "Remote"
    industry: "Big Data"
    key_responsibilities:
      - "Led the creation of a Domain Data Warehouse with Hadoop, Hive, and Spark, transitioning from Oracle Mainframe."
      - "Revamped legacy SQL scripts, achieving a 25% improvement in data processing efficiency."
      - "Streamlined ETL processes, achieving a 98% data integrity rate."
      - "Led cloud migration projects using Azure, to scale data processing by 5x, reducing processing times by 40%, and improving overall system efficiency by 35%."
      - "Developed and executed data validation frameworks, significantly reducing data discrepancies and errors by 30%."
      - "Architected scalable data ingestion services capable of processing billions of events daily, enabling real-time reporting and analytics."
      - "Mentored junior team members in data engineering best practices, resulting in improved team productivity and a 50% reduction in onboarding time."
    skills_acquired:
      - "Apache Hive (2+ years)"
      - "Hadoop (2+ years)"
      - "Python (2+ years)"
      - "SQL (4+ years)"
      - "ETL Optimization"
      - "Data Warehousing"
      - "Big Data Tools"
      - "Data Integration"
      - "Cloud Computing (Azure, OCI)"
      - "Docker (Containerization)"
      - "Kubernetes (Orchestration)"
      - "CI/CD Pipelines (GitLab CI)"
      - "Data Pipeline Automation"
      - "Data Governance"
      - "Data Security & Privacy"
      - "Real-time Data Processing (Kafka)"
      - "API Development"
      - "Microservices Architecture (in the context of data)"
      - "Data Lakes"
      - "Data Quality Monitoring"
      - "Message Queuing (Kafka)"
      - "Version Control (Git, GitLab)"
      - "Scrum/Agile Methodologies"
      - "Data Orchestration Tools (Apache Airflow)"
      - "Performance Tuning & Optimization"
      - "Streaming Analytics"
      - "Automated Testing (Unit, Integration Tests)"
      - "Distributed Computing Systems"
      - "Azure Synapse Analytics"
      - "Data Pipeline Optimization"
      - "Cloud Migration"

  - position: "Team Lead / Data Engineer"
    company: "SITONICA"
    employment_period: "09/2020 - 11/2021"
    location: "Remote"
    industry: "Data Engineering"
    key_responsibilities:
      - "Led critical projects as Team Lead, achieving a 100% increase in data processing capabilities through advanced server integration."
      - "Managed data storage expansions, tripling storage capacity and access speed."
      - "Oversaw complex data migrations, improving database system performance and reliability by 150%."
      - "Designed and implemented a big data architecture with Azure Data Lake, increasing system scalability by 200% and reducing data storage costs by 25%. Enabled real-time analytics that sped up decision-making processes by 40%."
      - "Implemented automated alerting systems for critical data pipeline components, minimizing downtime by over 60%."
      - "Engineered advanced data partitioning strategies, achieving faster query performance and 35% efficiency gains in storage utilization."
      - "Led adoption of infrastructure-as-code practices (Terraform), accelerating environment provisioning by 70% and enhancing infrastructure consistency."
    skills_acquired:
      - "Python (1+ year)"
      - "Data Storage"
      - "Data Migrations"
      - "Database Design"
      - "SQL"
      - "ETL Processes"
      - "Server Administration"
      - "Cloud Computing (Azure, OCI)"
      - "Docker (Containerization)"
      - "CI/CD Pipelines (GitLab CI)"
      - "Data Governance"
      - "Data Security & Privacy"
      - "Real-time Data Processing (Kafka)"
      - "API Development"
      - "Microservices Architecture (in the context of data)"
      - "Data Lakes"
      - "Data Quality Monitoring"
      - "Message Queuing (Kafka)"
      - "Version Control (Git, GitLab)"
      - "Scrum/Agile Methodologies"
      - "Data Orchestration Tools (Apache Airflow)"
      - "Performance Tuning & Optimization"
      - "Automated Testing (Unit, Integration Tests)"
      - "Distributed Computing Systems"
      - "Azure Data Lake"
      - "Big Data Architecture"
      - "Cost Optimization"

  - position: "Prompt Engineer"
    company: "AI Research Collaborative"
    employment_period: "12/2022 - Present"
    location: "Remote"
    industry: "Artificial Intelligence / Machine Learning"
    key_responsibilities:
      - "Designed and implemented prompt engineering strategies to optimize interactions with large language models, enhancing response accuracy and relevance."
      - "Developed custom prompts for various applications, including data analysis, report generation, and automated support systems."
      - "Collaborated with data engineering teams to integrate AI-driven solutions into existing data pipelines, improving overall system intelligence."
      - "Conducted research on the latest advancements in prompt engineering, applying best practices to continuously improve model performance."
      - "Pioneered a structured methodology for prompt optimization, increasing accuracy of AI-generated outputs by 25%."
      - "Automated end-to-end workflows integrating AI model responses directly into client-facing applications, reducing manual intervention by 40%."
      - "Regularly conducted internal training sessions on prompt engineering techniques, significantly increasing team proficiency and innovative capabilities."
    skills_acquired:
      - "Prompt Engineering"
      - "Natural Language Processing (NLP)"
      - "Large Language Models (LLMs)"
      - "AI Integration"
      - "Machine Learning Basics"
      - "Python for AI"
      - "API Integration (OpenAI, etc.)"
      - "Automated Report Generation"
      - "AI-driven Data Analysis"
      - "Research and Development in AI"

volunteer_experience:
  - position: "Volunteer Animal Rescuer"
    organization: "Animal Welfare Volunteers"
    period: "08/2023 - Present"
    location: "Georgia"
    description: |
      Rescued and rehabilitated homeless cats in Tbilisi, arranging medical care, daily feeding, grooming, and adoption coordination. Raised community awareness on responsible pet ownership and organized small‑scale fundraising for veterinary expenses.

projects:
  - name: "News Analysis and Translation Telegram Bot"
    description: "Developed an advanced Telegram bot that aggregates, translates, and intelligently analyzes news from Montenegrin sources. Leveraging cutting-edge NLP techniques, including BERT for deduplication and Azure Cognitive Services for automated summarization, the bot delivers concise and timely news digests. This solution is currently the first and only AI-powered comprehensive news aggregation system for Montenegro."
    link: "https://github.com/queukat/news_parcer"

  - name: "Spark Universal Migrator"
    description: "The Spark Universal Migrator is a highly efficient data migration application designed to transfer data from Oracle to Hive using Apache Spark. It ensures maximum performance by leveraging JDBC connections and parallel processing with ThreadPoolExecutor. The application handles complex data migration cases, including type conversion and partitioning, while preserving data integrity and completeness."
    link: "https://mvnrepository.com/artifact/io.github.queukat/oracletohivemigrator_2.12/2.0"

achievements:
  - name: "Significant Data Optimization"
    description: "Achieved up to a 50% reduction in data processing times across multiple projects."
  - name: "Leadership in Team Management"
    description: "Recognized for leading cross-functional teams, resulting in a 100% increase in project completion rate."

certifications:
  - name: "Oracle Cloud Data Management 2023 Certified Foundations Associate"
    description: "Focused on core Oracle Cloud Data Management concepts, including database services, database deployment options, and basic management tasks, preparing for foundational knowledge in managing Oracle Cloud Databases."
  - name: "Oracle Cloud Infrastructure 2023 Certified Foundations Associate"
    description: "Verified knowledge of essential Oracle Cloud Infrastructure services, including computing, storage, networking, and database services, with a focus on cloud architecture and management."
  - name: "Data Architect (Udacity)"
    description: "Nanodegree program teaching essential skills for designing, building, and managing large-scale data architecture, focusing on data modeling, cloud-based data warehousing, and ETL pipeline management."
  - name: "Data Engineer"
    description: "Validated expertise in designing and managing data pipelines, building data processing systems, and working with big data technologies such as Apache Hadoop, Apache Spark, and cloud platforms."
  - name: "Python Developer"
    description: "Demonstrates proficiency in Python programming, covering object-oriented programming, data structures, web frameworks, and API development."
  - name: "Oracle Cloud Infrastructure 2025 Certified AI Foundations Associate"
    description: "Introduces foundational concepts of Artificial Intelligence (AI) and Machine Learning (ML) within Oracle Cloud Infrastructure (OCI), covering supervised and unsupervised learning, deep learning models like CNNs and RNNs, and OCI's AI tools including Generative AI and Oracle 23ai services."
  - name: "Microsoft Certified: Fabric Data Engineer Associate"
    issued: "Jul 2025"
    description: "Validates expertise in designing and implementing end‑to‑end data engineering solutions on Microsoft Fabric, including Lakehouse architecture, Data Engineering pipelines, and Power BI integrations."

languages:
  - language: "English"
    proficiency: "Professional"
    short_name: "en"

interests:
  - "Big Data Analytics"
  - "Machine Learning"
  - "Cloud Computing"
  - "Animal Welfare"

professional_summary:
  - "Experienced Senior Data Engineer with 7+ years in Big Data, Cloud, and AI-driven platforms. Proven success in optimizing data pipelines (up to 50% faster), leading Azure migrations, and integrating Snowflake & Databricks workloads. Passionate about leveraging machine learning and prompt engineering to drive strategic insights. Certified in Oracle Cloud, Microsoft Fabric, and Python Development, with strong leadership and mentoring skills."

availability:
  notice_period: "immediately"
  days: "5"

salary_expectations:
  salary_range_usd: "45000-50000"

self_identification:
  gender: "Male"
  pronouns: "He/Him"
  veteran: "No"
  disability: "No"
  ethnicity: "White"

legal_authorization:
  eu_work_authorization: "No"
  us_work_authorization: "No"
  requires_us_visa: "Yes"
  requires_us_sponsorship: "Yes"
  requires_eu_visa: "Yes"
  legally_allowed_to_work_in_eu: "No"
  legally_allowed_to_work_in_us: "No"
  requires_eu_sponsorship: "Yes"
  canada_work_authorization: "No"
  requires_canada_visa: "Yes"
  legally_allowed_to_work_in_canada: "No"
  requires_canada_sponsorship: "Yes"
  uk_work_authorization: "Yes"
  requires_uk_visa: "No"
  legally_allowed_to_work_in_uk: "Yes"
  requires_uk_sponsorship: "No"

work_preferences:
  remote_work: "Yes"
  in_person_work: "Yes"
  open_to_relocation: "Yes"
  willing_to_complete_assessments: "Yes"
  willing_to_undergo_drug_tests: "Yes"
  willing_to_undergo_background_checks: "Yes"

